{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we say, black box from perspective of function such as f(x)=ax+b only have two parameters also in chatGPT model have 175 billion parameters. compare and surprise. Now, show f(x) to form of Artificial Neural Network. \n",
    "\n",
    "![ANN](../AI_Images/Neuron.jpg)\n",
    "\n",
    "Above figure show ANN with two input and in the real case input more than 2. Another ANN as shown in the following that f(x) related to n feaure of input. \n",
    "\n",
    "![ANN2](../AI_Images/ANN2.jpg)\n",
    "\n",
    "for example ANN able to segment main image to some parts which name segment. \n",
    "\n",
    "![Segmentation_ANN](../AI_Images/Segmentation_ANN.jpg)\n",
    "\n",
    "YOLO creat for object detection and classification. If we used YOLO architecture for image of mnist 28*28 in one case, it have 421642 parametrs \n",
    "\n",
    "![Yolo_ObjectDetectionClassification](../AI_Images/YOLO1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep neural networks (DNNs)** significantly enhance image processing tasks like sharpening, enhancement, background subtraction, and substitution. Examples include:\n",
    "\n",
    "1. **Image Sharpening**: Convolutional neural networks (CNNs) such as SRCNN increase resolution and detail in images.\n",
    "2. **Image Enhancement**: Techniques like Denoising Autoencoders (DAEs) and Generative Adversarial Networks (GANs) improve image quality by removing noise and enhancing features.\n",
    "3. **Background Subtraction**: Networks like FgSegNet accurately segment and subtract the background in videos.\n",
    "4. **Background Substitution**: Models like DeepLab and U-Net facilitate high-quality background replacement in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Sardar1](../AI_Images/Sardar1.JPG)\n",
    "\n",
    "![Sardar1_DeepAI1](../AI_Images/Sardar1_1.jpg)\n",
    "\n",
    "![Sardar1_DeepAI2](../AI_Images/Sardar1_2.jpg)\n",
    "\n",
    "![Sardar2](../AI_Images/Sardar2.JPG)\n",
    "\n",
    "![Sardar2_AI1](../AI_Images/Sardar2_1.png)\n",
    "\n",
    "![Sardar3](../AI_Images/Sardar3.JPG)\n",
    "\n",
    "![Sardar3_AI1](../AI_Images/Sardar3_Enhaunce_1.jpg)\n",
    "\n",
    "![ABooMahdi](../AI_Images/Aboomahdi_main.jpg)\n",
    "\n",
    "![ABooMahdi_AI](../AI_Images/Aboomahdi.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bit of fun**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Main_Hadi](../AI_Images/Hadi.jpg)\n",
    "\n",
    "![Hadi_Enhaunce](../AI_Images/Hadi-denoised_sharpened.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change Background**\n",
    "\n",
    "![MainHadi](../AI_Images/hadisadoghiyazdi.jpg)\n",
    "\n",
    "![Hadi_Background](../AI_Images/hadisadoghiyazdiBackground.jpg)\n",
    "\n",
    "![FireMan](../AI_Images/FireMan.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside Deep Neural Network\n",
    "The emphasized part of the image, which the neural network focuses on for animal classification using the ResNet network trained on ImageNet.\n",
    "\n",
    "![InsideDeep1](../AI_Images/Resnet_Trained_ImageNet_Animal.JPG)\n",
    "\n",
    "### [WordNet](https://en.wikipedia.org/wiki/WordNet)\n",
    "The database contains 155,327 words organized in 175,979 synsets (sets of cognitive synonyms) for a total of 207,016 word-sense pairs; in compressed form, it is about 12 megabytes in size.\n",
    "\n",
    "![Example WordNet](../AI_Images/WordNetExample_1.JPG)\n",
    "\n",
    "**Related Forms and Semantic Relations:**\n",
    "\n",
    "Synonyms: glad, joyful, jubilant\n",
    "\n",
    "Antonyms: sad, unhappy, sorrowful\n",
    "\n",
    "Related Words: elated, cheerful, blissful, delighted\n",
    "\n",
    "Hypernyms (more general terms): emotional, feeling\n",
    "\n",
    "Hyponyms (more specific terms): blissful, euphoric, overjoyed\n",
    "\n",
    "### [ImageNet](https://www.image-net.org/)\n",
    "[ImageNet](https://ieeexplore.ieee.org/document/5206848) is an image dataset organized according to the WordNet hierarchy average 1000 images to illustrate each synset. ImageNet-21k contains 14,197,122 annotated images divided into 21,841 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dream and Art\n",
    "\n",
    "[DeepDream](https://deepdreamgenerator.com/)\n",
    "\n",
    "![DeepDream1](../AI_Images/DeepDream1.JPG)\n",
    "\n",
    "![DeepDream1](../AI_Images/DeepDream2.JPG)\n",
    "\n",
    "![DeepDream1](../AI_Images/DeepDream3.JPG)\n",
    "\n",
    "![DeepDream1](../AI_Images/DeepDream4.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Inside surgery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep litte carefull\n",
    "A deep neural network effectively acts as an **information distillation pipeline**, with raw data going in (in this case, RGB pictures) and being repeatedly transformed so that irrelevant information is filtered out (for example, the specific visual appearance of the image), and\n",
    "useful information is magnified and refined (for example, the class of the image). \n",
    "\n",
    "![Distillation](../AI_Images/Distillation.jpg)\n",
    "\n",
    "**_There are a few things to note here:_**\n",
    "- The first layer acts as a collection of various edge detectors. At that stage, the activations retain almost all of the information present in the initial picture. As you go deeper, the activations become increasingly abstract and less visually interpretable. They begin to encode higher-level concepts such as “cat ear” and “cat eye.” \n",
    "- Deeper presentations carry increasingly less information about the visual contents of the image, and increasingly more information related to the class of the image.\n",
    "- The sparsity of the activations increases with the depth of the layer: in the first layer, almost all filters are activated by the input image, but in the following layers, more and more filters are blank. This means the pattern encoded by the filter isn’t found in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-visualization\n",
    "Similar to **Brain Mapping**\n",
    "- Areas of the brain that control various functions of the body by stimulating specific points of the brain\n",
    "\n",
    "![Brain Mapping](../AI_Images/BrainMapping.JPG)\n",
    "\n",
    "**Functional Areas:** The map highlights which regions of the brain control various functions, such as movement, sensation, speech, and vision.\n",
    "\n",
    "![Brain Mapping Illustration](../AI_Images/BrainMap_1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 A Type Of Deep\n",
    "![VGG16](../AI_Images/VGG16.JPG)\n",
    "\n",
    "for Xception model\n",
    "\n",
    "![Cat](../AI_Images/cat.JPG)\n",
    "\n",
    "![CatFilter](../AI_Images/FirstLayer_Fifth_Filter.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Input (X) Excite specific layer\n",
    "**Response** :Feature Visualization\n",
    "\n",
    "\n",
    "Second Channel is excited optimality using this pattern\n",
    "\n",
    "![ExcitedSecondChannel](../AI_Images/Pattern_secondChannel_RespondsMaximally.JPG)\n",
    "\n",
    "Block 4 Conv 1 is excited optimality by:\n",
    "\n",
    "![Block4_Conv1](../AI_Images/block4_sepconv1.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet trained on the ImageNet.\n",
    "\n",
    "![GoogleNet_ImageNet](../AI_Images/GoogleNet_ImageNet.JPG)\n",
    "\n",
    "Type of neural network and the data it is trained on significantly affect feature visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Feature Visualization in Neural Networks\n",
    "\n",
    "- Feature visualization in neural networks is a method for exploring the internal mechanisms of deep neural networks.\n",
    "- Method: Identifying an input that maximizes the output of a specific layer, we can define the optimization as finding an input that maximizes a target layer's output.\n",
    "- This approach allows us to understand what kind of stimuli activate particular points within the intricate structure of the deep network.\n",
    "- Deeper layers often respond to input concepts that are more abstract and closer to human interpretation, indicating that these deep points are triggered by inputs with richer semantic meaning.\n",
    "- **Fusion** of Simultaneous Layers: Optimal Way to Specific Input Leading to DeepDream"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
